{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Solr OCR Highlighting This Solr plugin lets you put word-level OCR text into one or more of you documents\u2019 fields and then allows you to obtain structured highlighting data with the text and its position on the page at query time: { \"text\": \"to those parts, subject to unreasonable claims from the pro\u00adprietor \" \"of Maryland, until the year 17C2, when the whole controversy was \" \"settled by Charles <em>Mason and Jeremiah Dixon</em>, upon their \" \"return from an observation of the tran\u00adsit of Venus, at the Cape of \" \"Good Hope, where they\", \"score\": 5555104.5, \"pages\": [ { \"id\": \"page_380\", \"width\": 1436, \"height\": 2427 } ], \"regions\": [ { \"ulx\": 196, \"uly\": 1703, \"lrx\": 1232, \"lry\": 1968, \"pageIdx\": 0 } ], \"highlights\":[ [{ \"text\": \"Mason and Jeremiah\", \"ulx\": 675, \"uly\": 110, \"lrx\": 1036, \"lry\": 145, \"parentRegionIdx\": 0}, { \"text\": \"Dixon,\", \"ulx\": 1, \"uly\": 167, \"lrx\": 119, \"lry\": 204, \"parentRegionIdx\": 0 }] ] } All this is done without having to store any OCR text in the index itself : The plugin can lazy-load only the parts required for highlighting at both index and query time from your original OCR input documents. It works by extending Solr\u2019s standard UnifiedHighlighter with support for loading external field values and determining OCR positions from those field values. This means that most options and query types supported by the UnifiedHighlighter are also supported for OCR highlighting. The plugin also does not interfere with Solr\u2019s standard highlighting component, i.e. it works transparently with non-OCR fields and just lets the default implementation handle those. The plugin works with all Solr versions >= 7.x . Features Index various OCR formats directly without any pre-processing hOCR ALTO MiniOCR Retrieve all the information needed to render a highlighted snippet view directly from Solr, without post-processing Keep your index size manageable by optionally re-using OCR documents on disk for highlighting Getting Started To get started setting up OCR highlighting for your Solr server, head over to the Installation Instructions . If you want to see the plugin in action , you can play around with the example setup hosted at https://ocrhl.jbaiter.de Should you want to run the example on your own computer and play around with the settings, the Docker-based setup is available on GitHub and instructions for using it are in the Example Setup chapter","title":"Introduction"},{"location":"#solr-ocr-highlighting","text":"This Solr plugin lets you put word-level OCR text into one or more of you documents\u2019 fields and then allows you to obtain structured highlighting data with the text and its position on the page at query time: { \"text\": \"to those parts, subject to unreasonable claims from the pro\u00adprietor \" \"of Maryland, until the year 17C2, when the whole controversy was \" \"settled by Charles <em>Mason and Jeremiah Dixon</em>, upon their \" \"return from an observation of the tran\u00adsit of Venus, at the Cape of \" \"Good Hope, where they\", \"score\": 5555104.5, \"pages\": [ { \"id\": \"page_380\", \"width\": 1436, \"height\": 2427 } ], \"regions\": [ { \"ulx\": 196, \"uly\": 1703, \"lrx\": 1232, \"lry\": 1968, \"pageIdx\": 0 } ], \"highlights\":[ [{ \"text\": \"Mason and Jeremiah\", \"ulx\": 675, \"uly\": 110, \"lrx\": 1036, \"lry\": 145, \"parentRegionIdx\": 0}, { \"text\": \"Dixon,\", \"ulx\": 1, \"uly\": 167, \"lrx\": 119, \"lry\": 204, \"parentRegionIdx\": 0 }] ] } All this is done without having to store any OCR text in the index itself : The plugin can lazy-load only the parts required for highlighting at both index and query time from your original OCR input documents. It works by extending Solr\u2019s standard UnifiedHighlighter with support for loading external field values and determining OCR positions from those field values. This means that most options and query types supported by the UnifiedHighlighter are also supported for OCR highlighting. The plugin also does not interfere with Solr\u2019s standard highlighting component, i.e. it works transparently with non-OCR fields and just lets the default implementation handle those. The plugin works with all Solr versions >= 7.x .","title":"Solr OCR Highlighting"},{"location":"#features","text":"Index various OCR formats directly without any pre-processing hOCR ALTO MiniOCR Retrieve all the information needed to render a highlighted snippet view directly from Solr, without post-processing Keep your index size manageable by optionally re-using OCR documents on disk for highlighting","title":"Features"},{"location":"#getting-started","text":"To get started setting up OCR highlighting for your Solr server, head over to the Installation Instructions . If you want to see the plugin in action , you can play around with the example setup hosted at https://ocrhl.jbaiter.de Should you want to run the example on your own computer and play around with the settings, the Docker-based setup is available on GitHub and instructions for using it are in the Example Setup chapter","title":"Getting Started"},{"location":"alternatives/","text":"Support for alternative forms All OCR formats supported by this plugin have the possibility of encoding alternative readings for a given word. These can either come from the OCR engine itself and consist of other high-confidence readings for a given sequence of characters, or they could come from an manual or semi-automatic OCR correction system. Note For hOCR , use <span class=\"alternatives\"><ins class=\"alt\">...</ins><del class=\"alt\">...</del></span> (see hOCR specification ) For ALTO , use <String \u2026><ALTERNATIVE>...</ALTERNATIVE></String> (see AlternativeType in the ALTO schema ) For MiniOCR , delimit alternative forms with \u21ff (U+21FF) (see MiniOCR documentation ) In any case, these alternative readings can improve your user\u2019s search experience, by allowing us to index multiple forms for a given text position . This enables users to find more matching passages for a given query than if only a single form was indexed for every word. This is a form of index-time term expansion , similar in concept to e.g. the Synonym Graph Filter that ships with Solr. To enable the indexing of alternative readings , you have to make some modifications to your OCR field\u2019s index analysis chain . First, you need to enable alternative expansion in the OcrCharFilterFactory by setting the expandAlternatives attribute to true : <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.OcrCharFilterFactory\" expandAlternatives=\"true\" /> Next, you need to add a new OcrAlternativesFilterFactory token filter component to your analysis chain. This component must to be placed after the tokenizer : <fieldType name=\"text_ocr\" class=\"solr.TextField\"> <!-- .... --> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"de.digitalcollections.solrocr.lucene.OcrAlternativesFilterFactory\"/> <!-- .... --> </fieldType> A full field definition for an OCR field with alternative expansion could look like this: <fieldType name=\"text_ocr\" class=\"solr.TextField\"> <analyzer type=\"index\"> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.ExternalUtf8ContentFilterFactory\"/> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.OcrCharFilterFactory\" expandAlternatives=\"true\" /> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"de.digitalcollections.solrocr.lucene.OcrAlternativesFilterFactory\"/< <filter class=\"solr.LowerCaseFilterFactory\"/> </analyzer> <analyzer type=\"query\"> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"solr.LowerCaseFilterFactory\"/> </analyzer> </fieldType> Highlighting matches on alternative forms During highlighting, you will only see the matching alternative form in the snippet if the match is on a single word, or if it is at the beginning or the end of a phrase match. This is because we cannot get to the offsets of matching terms inside of a phrase match through Lucene\u2019s highlighting machinery. Unsupported tokenizers The OcrAlternativesFilterFactory works with almost all tokenizers shipping with Solr, except for the ClassicTokenizer . This is because we use the WORD JOINER (U+2060) character to denote alternative forms in the character stream and the classic tokenizer splits tokens on this character (contrary to Unicode rules). This also means that if you use a custom tokenizer, you need to make sure that it does not split tokens on U+2060. Non-alphabetic characters in alternatives Some of Solr\u2019s built-in tokenizers split tokens on special characters like - that occur inside of words. When such characters occur within tokens that have alternatives, the alternatives are severed from the original token and the plugin will not index them. To avoid this, either use a tokenizer that doesn\u2019t split on these characters (like WhitespaceTokenizerFactory ) or consider customizing your tokenizer of choice to not split on these characters when a token includes alternative readings. Note that this can lead to less precise results, e.g. when alpha-numeric is not split, only a query like alphanumeric or alpha-numeric will match (depending on the analysis chains), but not alpha or numeric alone or a \"alpha numeric\" phrase query. Consider increasing the standard maxTokenLength of 255 When your OCR contains a large number of alternatives for tokens, or these alternatives can get quite long, consider increasing the maximum token length in your tokenizer\u2019s configuration. For most of Solr\u2019s tokenizers this can be done with the maxTokenLength parameter that defaults to 255. When the plugin encounters a case where this leads to truncated alternatives, it will print a warning to the Solr log. Consider increasing the value to 512 or 1024. This will come at the expense of an increase in memory usage during indexing, but will preserve as many of your alternative readings as possible.","title":"Indexing Alternative Terms"},{"location":"alternatives/#support-for-alternative-forms","text":"All OCR formats supported by this plugin have the possibility of encoding alternative readings for a given word. These can either come from the OCR engine itself and consist of other high-confidence readings for a given sequence of characters, or they could come from an manual or semi-automatic OCR correction system. Note For hOCR , use <span class=\"alternatives\"><ins class=\"alt\">...</ins><del class=\"alt\">...</del></span> (see hOCR specification ) For ALTO , use <String \u2026><ALTERNATIVE>...</ALTERNATIVE></String> (see AlternativeType in the ALTO schema ) For MiniOCR , delimit alternative forms with \u21ff (U+21FF) (see MiniOCR documentation ) In any case, these alternative readings can improve your user\u2019s search experience, by allowing us to index multiple forms for a given text position . This enables users to find more matching passages for a given query than if only a single form was indexed for every word. This is a form of index-time term expansion , similar in concept to e.g. the Synonym Graph Filter that ships with Solr. To enable the indexing of alternative readings , you have to make some modifications to your OCR field\u2019s index analysis chain . First, you need to enable alternative expansion in the OcrCharFilterFactory by setting the expandAlternatives attribute to true : <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.OcrCharFilterFactory\" expandAlternatives=\"true\" /> Next, you need to add a new OcrAlternativesFilterFactory token filter component to your analysis chain. This component must to be placed after the tokenizer : <fieldType name=\"text_ocr\" class=\"solr.TextField\"> <!-- .... --> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"de.digitalcollections.solrocr.lucene.OcrAlternativesFilterFactory\"/> <!-- .... --> </fieldType> A full field definition for an OCR field with alternative expansion could look like this: <fieldType name=\"text_ocr\" class=\"solr.TextField\"> <analyzer type=\"index\"> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.ExternalUtf8ContentFilterFactory\"/> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.OcrCharFilterFactory\" expandAlternatives=\"true\" /> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"de.digitalcollections.solrocr.lucene.OcrAlternativesFilterFactory\"/< <filter class=\"solr.LowerCaseFilterFactory\"/> </analyzer> <analyzer type=\"query\"> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"solr.LowerCaseFilterFactory\"/> </analyzer> </fieldType> Highlighting matches on alternative forms During highlighting, you will only see the matching alternative form in the snippet if the match is on a single word, or if it is at the beginning or the end of a phrase match. This is because we cannot get to the offsets of matching terms inside of a phrase match through Lucene\u2019s highlighting machinery. Unsupported tokenizers The OcrAlternativesFilterFactory works with almost all tokenizers shipping with Solr, except for the ClassicTokenizer . This is because we use the WORD JOINER (U+2060) character to denote alternative forms in the character stream and the classic tokenizer splits tokens on this character (contrary to Unicode rules). This also means that if you use a custom tokenizer, you need to make sure that it does not split tokens on U+2060. Non-alphabetic characters in alternatives Some of Solr\u2019s built-in tokenizers split tokens on special characters like - that occur inside of words. When such characters occur within tokens that have alternatives, the alternatives are severed from the original token and the plugin will not index them. To avoid this, either use a tokenizer that doesn\u2019t split on these characters (like WhitespaceTokenizerFactory ) or consider customizing your tokenizer of choice to not split on these characters when a token includes alternative readings. Note that this can lead to less precise results, e.g. when alpha-numeric is not split, only a query like alphanumeric or alpha-numeric will match (depending on the analysis chains), but not alpha or numeric alone or a \"alpha numeric\" phrase query. Consider increasing the standard maxTokenLength of 255 When your OCR contains a large number of alternatives for tokens, or these alternatives can get quite long, consider increasing the maximum token length in your tokenizer\u2019s configuration. For most of Solr\u2019s tokenizers this can be done with the maxTokenLength parameter that defaults to 255. When the plugin encounters a case where this leads to truncated alternatives, it will print a warning to the Solr log. Consider increasing the value to 512 or 1024. This will come at the expense of an increase in memory usage during indexing, but will preserve as many of your alternative readings as possible.","title":"Support for alternative forms"},{"location":"changes/","text":"0.7.2 (2022-03-22) And yet another bugfix release. Bugfixes: - Fixed using single-quotes in MiniOCR input, previously these files were not recognized as valid MiniOCR files ( #247 , thanks @mspalti for the fix!) - Fixed OutOfBoundsException when using alternatives with very long tokens ( #230 , thanks @fd17 for the report and review) 0.7.1 (2021-09-24) Another bugfix release, upgrading is recommended. Bugfixes: - Fix text display and \u201cnumber of snippets\u201d slider in demo setup - Fix instances where we were using Java SDK methods that relied on a default locale, which led to hard-to-debug issues in some locales - Fix an issue where a highlight rectangle would sometimes be oversized - Fix issue in XML input validation when encountering very long XML opening tags - Really fix handling of documents with no content (at all) - Fix issue with namespaced ALTO documents 0.7.0 (2021-07-12) This is a bugfix release, especially users with ALTO files are encouraged to upgrade. Other than bugfixes, this is the first release to support Solr 8.9. Bugfixes: Fix handling of documents with no content Fix issue with the handling of unicode-encoded files Fix issue when an hOCR file had empty OCR boxes 0.6.0 (2021-05-11) This is a major new release with significant improvements in stability, accuracy and most importantly performance. Updating is highly recommended, especially for ALTO users, who can expect a speed-up in indexing of up to 6000% (i.e. 60x as fast) . We also recommend updating your JVM to at least Java 11 (LTS), since Java 9 introduced a feature that speeds up highlighting significantly. Performance: Indexing performance drastically improved for ALTO, slightly worse for hOCR and MiniOCR. Under the hood we switched from Regular Expression and Automaton-based parsing to a proper XML parser to support more features and improve correctness. This drastically improved indexing performance for ALTO (6000% speedup, the previous implementation was pathologically slow), but caused a big hit for hOCR (~57% slower) and a slight hit for MiniOCR (~15% slower). These numbers are based on benchmarks done on a ramdisk, so the changes are very likely to be less pronounced in practice, depending on the choice of storage. Note that this makes the parser also more strict in regard to whitespace. If you were indexing OCR documents without any whitespace between word elements before, you will run into problems (see #147 ). Highlighting performance significantly improved for all formats. The time for highlighting a single snippet has gone down for all formats (ALTO 12x as fast, hOCR 10x as fast, MiniOCR 6x as fast). Again, these numbers are based on benchmarks performed on a ramdisk and might be less pronounced in practice, depending on the storage layer. New Features: Indexing alternative forms encoded in the source OCR files. All supported formats offer a way to encode alternative readings for recognized words. The plugin can now parse these from the input files and index them at the same position as the default form. This is a form of index-time term expansion (much like the Synonym Graph Filter shipping with Solr). For example, if you OCR file has the alternatives christmas and christrias for the token clistrias in the span presents on clistrias eve , users would be able to search for \"presents christmas\" and \"presents clistrias\" and would get the correct match in both cases, both with full highlighting. Refer to the corresponding section in the documentation for instructions on setting it up. On-the-fly repair of \u2018broken\u2019 markup. OcrCharFilterFactory has a new option fixMarkup that enables on-the-fly repair of invalid XML in OCR input documents, namely problems that can arise when the markup contains unescaped instances of < , > and & . This option is disabled by default, we recommend enabling it when your OCR engine exhibits this problem and you are unable to fix the files on disk, since it incurs a bit of a performance hit during indexing. Return snippets in order of appearance . By default, Solr scores each highlighted passage as a \u201cmini-document\u201d and returns the passages ordered by their decreasing score. While this is a good match for a lot of use cases, there are many other that are better suited with a simple by-appearance order. This can now be controlled with the new hl.ocr.scorePassages parameter, which will switch to the by-appearance sort order if set to off (it is set to on by default) API changes: - No more need for an explicit hl.fl parameter for highlighting non-OCR fields. By default, if highlighting is enabled and no hl.fl parameter is passed by the user, Solr falls back to highlighting every stored field in the document. Previously this did not work with the plugin and users had to always explicitly specify which fields they wanted to have highlighted. This is no longer neccessary , the default behavior now works as expected. - Add a new hl.ocr.trackPages parameter to disable page tracking during highlighting. This is intended for users who index one page per document, in these cases seeking backwards to determine the page identifier a match is not needed, since the containing document contains enough information to identify the page, improving highlighting performance due to the need for less backwards-seeking in the input files. - Add new expandAlternatives attribute to OcrCharFilterFactory . This enables the parsing of alternative readings from input files (see above and the corresponding section in the documentation ) - Add new hl.ocr.scorePassages parameter to disable sorting of passages by their score. See the above section unter New Features for an explanation of this flag. Bugfixes: - Improved tolerance for incomplete bounding boxes. Previously the occurrence of an incomplete bounding box in a snippet (i.e. with one or more missing coordinates) would crash the whole query. We now simply insert a 0 default value in these cases. - Improvements in the handling of hyphenated terms. This release fixes a few bugs in edge cases when handling hyphenated words during indexing, highlighting and snippet text generation. - Handle empty field values during indexing. This would previously lead to an exception since the OCR parsers would try to either load a file from the empty string or parse OCR markup from it. 0.5.0 (2020-10-07) No breaking changes this time around, but a few essential bugfixes, more stability and a new feature. API changes: - Snippets are now sorted by their descending score/relevancy. Previously the order was non-deterministic, which broke the use case for dynamically fetching more snippets. - Add a new boolean hl.ocr.alignSpans parameter to align text and image spans. This new option (disabled by default) ensures that the spans in text and image match, i.e. it forces the <em>...</em> in the highlighted text to correspond to actual OCR word boundaries. Bugfixes: - Fix regular highlighting in distributed setup. Regular, non-OCR highlighting was broken in previous versions due to a bad check in the shard response collection phase if users only requested regular highlighting, but not for OCR fields - Highlight spans are now always consistent with the spans designated in text. Due to a bug, it would sometimes happen that the number of spans was inconsistent between the two. - Fix de-hyphenation in ALTO region texts. Previously only the complete snippet text would be de-hyphenated, but not the individual regions. - Fix post-match content detection in ALTO. A bug in this part of the code resulted in crashes when highlighting certain ALTO documents. 0.4.1 (2020-06-02) This is a patch release with a fix for excessive memory usage during indexing. 0.4.0 (2020-05-11) This is a major release with a focus on compatibility and performance. Fixes compatibility with Solr/Lucene 8.4 and 7.6 . We now also have an integration test suite that checks for compatibility with all Solr versions >= 7.5 on every change, so compatibility breakage should be kept to a minimum in the future. Breaking API changes: - Add new pages key to snippet response with page dimensions . This can be helpful if you need to calculate the snippet coordinates relative to the page image dimensions. - Replace page key on regions and highlights with pageIdx . That is, instead of a string with the corresponding page identifier, we have a numerical index into the pages array of the snippett. This reduces the redundancy introduced by the new pages parameter at the cost of having to do some pointer chasing in clients. - Add new parentRegionIdx key on highlights. This is a numerical index into the regions array and allows for multi-column/multi-page highlighting, where a single highlighting span can be composed of regions on multiple disjunct parts of the page or even multiple pages. Format changes: - hocr: Add support for retrieving page identifier from x_source an ppageno properties - hocr: Strip out title tag during indexing and highlighting - ALTO: The plugin now supports ALTO files with coordinates expressed as floating point numbers (thanks to @mspalti!) Performance: - Add concurrent preloading for highlighting target files. This can result in a nice performance boost, since by the time the plugin gets to actually highlighting the files, their contents are already in the OS\u2019 page cache. See the Performance Tuning section in the docs for more context. - This release changes the way we handle UTF-8 during context generation, resulting in an additional ~25% speed up compared to previous versions. Miscellaneous: - Log warnings during source pointer parsing - Filter out empty files during indexing - Add new documentation section on performance tuning - Empty regions or regions with only whitespace are no longer included in the output 0.3.1 (2019-07-26) This is patch release that fixes compatibility with Solr/Lucene 8.2. 0.3 (2019-07-25) GitHub Release This release brings some sweeping changes across the codebase, all aimed at making the plugin much simpler to use and less complicated to maintain. However, this also means a lot of breaking changes . It\u2019s best to go through the documentation (which has been simplified and was largely rewritten) again and see what changes you need to apply to your setup. Specifying path resolving is no longer neccessary. You now pass a pointer to one or more files (or regions thereof) directly in the index document. The pointer will be stored with the document and used to locate the input file(s) during highlighting. Refer to the documentation for more details. This should also increase indexing performance and decrease the memory requirements , since at no point does the complete OCR document need to be held in memory. hl.weightMatches now works with UTF8 . You no longer need to ASCII-encode your OCR files to be able to use Solr\u2019s superior highlighting approach. Due to the first change, the plugin now takes care of mapping UTF8 byte-offsets to character offsets by itself. This also means all code related to storing byte offsets in payloads is gone. Specifying the OCR format is no longer neccessary. The plugin now offers a single OcrFormatCharFilter that will auto-detect the OCR format used for a given document and select the correct analysis chain. This means that using multiple OCR formats for the same field is now possible! Performance improvements. Some optimizations were done to the way the plugin seeks through the OCR files. You should see a substantial performance improvement for documents with a low density of multi-byte codepoints, especially English. Also included is a new hl.ocr.maxPassages parameter to control how many passages are looked at for building the response, which can have an enormous impact on performance. Major Breaking Changes : HighlightComponent is now called OcrHighlightComponent for more clarity OCR fields to be highlighted now need to be passed with the hl.ocr.fl parameter Auto-detection of highlightable fields is no longer possible with the standard highlighter, fields to be highlighted need to be passed explicitely with the hl.fl parameter In the order of components, the OCR highlighting component needs to come before the standard highlighter to avoid conflicts. 0.2 (2019-07-16) GitHub Release Breaking Change : ALTO and hOCR now have custom CharFilter implementations that should be used instead of HTMLStripCharFilterFactory . Refer to the documentation for more details. Feature: Resolve Hyphenation at indexing time for all supported formats. If a word is broken across multiple lines, it will be indexed as the dehyphenated form. During highlighting, the parts on both lines will be highlighted appropriately. Fix calculation of passages with matches spanning multiple lines, in previous versions some passages would be too small Fix hl.fl parameter handling, a bug in 0.1 made this parameter not have any effect 0.1 (2019-06-06) GitHub Release Initial Release","title":"Change Log"},{"location":"changes/#072-2022-03-22","text":"And yet another bugfix release. Bugfixes: - Fixed using single-quotes in MiniOCR input, previously these files were not recognized as valid MiniOCR files ( #247 , thanks @mspalti for the fix!) - Fixed OutOfBoundsException when using alternatives with very long tokens ( #230 , thanks @fd17 for the report and review)","title":"0.7.2 (2022-03-22)"},{"location":"changes/#071-2021-09-24","text":"Another bugfix release, upgrading is recommended. Bugfixes: - Fix text display and \u201cnumber of snippets\u201d slider in demo setup - Fix instances where we were using Java SDK methods that relied on a default locale, which led to hard-to-debug issues in some locales - Fix an issue where a highlight rectangle would sometimes be oversized - Fix issue in XML input validation when encountering very long XML opening tags - Really fix handling of documents with no content (at all) - Fix issue with namespaced ALTO documents","title":"0.7.1 (2021-09-24)"},{"location":"changes/#070-2021-07-12","text":"This is a bugfix release, especially users with ALTO files are encouraged to upgrade. Other than bugfixes, this is the first release to support Solr 8.9. Bugfixes: Fix handling of documents with no content Fix issue with the handling of unicode-encoded files Fix issue when an hOCR file had empty OCR boxes","title":"0.7.0 (2021-07-12)"},{"location":"changes/#060-2021-05-11","text":"This is a major new release with significant improvements in stability, accuracy and most importantly performance. Updating is highly recommended, especially for ALTO users, who can expect a speed-up in indexing of up to 6000% (i.e. 60x as fast) . We also recommend updating your JVM to at least Java 11 (LTS), since Java 9 introduced a feature that speeds up highlighting significantly. Performance: Indexing performance drastically improved for ALTO, slightly worse for hOCR and MiniOCR. Under the hood we switched from Regular Expression and Automaton-based parsing to a proper XML parser to support more features and improve correctness. This drastically improved indexing performance for ALTO (6000% speedup, the previous implementation was pathologically slow), but caused a big hit for hOCR (~57% slower) and a slight hit for MiniOCR (~15% slower). These numbers are based on benchmarks done on a ramdisk, so the changes are very likely to be less pronounced in practice, depending on the choice of storage. Note that this makes the parser also more strict in regard to whitespace. If you were indexing OCR documents without any whitespace between word elements before, you will run into problems (see #147 ). Highlighting performance significantly improved for all formats. The time for highlighting a single snippet has gone down for all formats (ALTO 12x as fast, hOCR 10x as fast, MiniOCR 6x as fast). Again, these numbers are based on benchmarks performed on a ramdisk and might be less pronounced in practice, depending on the storage layer. New Features: Indexing alternative forms encoded in the source OCR files. All supported formats offer a way to encode alternative readings for recognized words. The plugin can now parse these from the input files and index them at the same position as the default form. This is a form of index-time term expansion (much like the Synonym Graph Filter shipping with Solr). For example, if you OCR file has the alternatives christmas and christrias for the token clistrias in the span presents on clistrias eve , users would be able to search for \"presents christmas\" and \"presents clistrias\" and would get the correct match in both cases, both with full highlighting. Refer to the corresponding section in the documentation for instructions on setting it up. On-the-fly repair of \u2018broken\u2019 markup. OcrCharFilterFactory has a new option fixMarkup that enables on-the-fly repair of invalid XML in OCR input documents, namely problems that can arise when the markup contains unescaped instances of < , > and & . This option is disabled by default, we recommend enabling it when your OCR engine exhibits this problem and you are unable to fix the files on disk, since it incurs a bit of a performance hit during indexing. Return snippets in order of appearance . By default, Solr scores each highlighted passage as a \u201cmini-document\u201d and returns the passages ordered by their decreasing score. While this is a good match for a lot of use cases, there are many other that are better suited with a simple by-appearance order. This can now be controlled with the new hl.ocr.scorePassages parameter, which will switch to the by-appearance sort order if set to off (it is set to on by default) API changes: - No more need for an explicit hl.fl parameter for highlighting non-OCR fields. By default, if highlighting is enabled and no hl.fl parameter is passed by the user, Solr falls back to highlighting every stored field in the document. Previously this did not work with the plugin and users had to always explicitly specify which fields they wanted to have highlighted. This is no longer neccessary , the default behavior now works as expected. - Add a new hl.ocr.trackPages parameter to disable page tracking during highlighting. This is intended for users who index one page per document, in these cases seeking backwards to determine the page identifier a match is not needed, since the containing document contains enough information to identify the page, improving highlighting performance due to the need for less backwards-seeking in the input files. - Add new expandAlternatives attribute to OcrCharFilterFactory . This enables the parsing of alternative readings from input files (see above and the corresponding section in the documentation ) - Add new hl.ocr.scorePassages parameter to disable sorting of passages by their score. See the above section unter New Features for an explanation of this flag. Bugfixes: - Improved tolerance for incomplete bounding boxes. Previously the occurrence of an incomplete bounding box in a snippet (i.e. with one or more missing coordinates) would crash the whole query. We now simply insert a 0 default value in these cases. - Improvements in the handling of hyphenated terms. This release fixes a few bugs in edge cases when handling hyphenated words during indexing, highlighting and snippet text generation. - Handle empty field values during indexing. This would previously lead to an exception since the OCR parsers would try to either load a file from the empty string or parse OCR markup from it.","title":"0.6.0 (2021-05-11)"},{"location":"changes/#050-2020-10-07","text":"No breaking changes this time around, but a few essential bugfixes, more stability and a new feature. API changes: - Snippets are now sorted by their descending score/relevancy. Previously the order was non-deterministic, which broke the use case for dynamically fetching more snippets. - Add a new boolean hl.ocr.alignSpans parameter to align text and image spans. This new option (disabled by default) ensures that the spans in text and image match, i.e. it forces the <em>...</em> in the highlighted text to correspond to actual OCR word boundaries. Bugfixes: - Fix regular highlighting in distributed setup. Regular, non-OCR highlighting was broken in previous versions due to a bad check in the shard response collection phase if users only requested regular highlighting, but not for OCR fields - Highlight spans are now always consistent with the spans designated in text. Due to a bug, it would sometimes happen that the number of spans was inconsistent between the two. - Fix de-hyphenation in ALTO region texts. Previously only the complete snippet text would be de-hyphenated, but not the individual regions. - Fix post-match content detection in ALTO. A bug in this part of the code resulted in crashes when highlighting certain ALTO documents.","title":"0.5.0 (2020-10-07)"},{"location":"changes/#041-2020-06-02","text":"This is a patch release with a fix for excessive memory usage during indexing.","title":"0.4.1 (2020-06-02)"},{"location":"changes/#040-2020-05-11","text":"This is a major release with a focus on compatibility and performance. Fixes compatibility with Solr/Lucene 8.4 and 7.6 . We now also have an integration test suite that checks for compatibility with all Solr versions >= 7.5 on every change, so compatibility breakage should be kept to a minimum in the future. Breaking API changes: - Add new pages key to snippet response with page dimensions . This can be helpful if you need to calculate the snippet coordinates relative to the page image dimensions. - Replace page key on regions and highlights with pageIdx . That is, instead of a string with the corresponding page identifier, we have a numerical index into the pages array of the snippett. This reduces the redundancy introduced by the new pages parameter at the cost of having to do some pointer chasing in clients. - Add new parentRegionIdx key on highlights. This is a numerical index into the regions array and allows for multi-column/multi-page highlighting, where a single highlighting span can be composed of regions on multiple disjunct parts of the page or even multiple pages. Format changes: - hocr: Add support for retrieving page identifier from x_source an ppageno properties - hocr: Strip out title tag during indexing and highlighting - ALTO: The plugin now supports ALTO files with coordinates expressed as floating point numbers (thanks to @mspalti!) Performance: - Add concurrent preloading for highlighting target files. This can result in a nice performance boost, since by the time the plugin gets to actually highlighting the files, their contents are already in the OS\u2019 page cache. See the Performance Tuning section in the docs for more context. - This release changes the way we handle UTF-8 during context generation, resulting in an additional ~25% speed up compared to previous versions. Miscellaneous: - Log warnings during source pointer parsing - Filter out empty files during indexing - Add new documentation section on performance tuning - Empty regions or regions with only whitespace are no longer included in the output","title":"0.4.0 (2020-05-11)"},{"location":"changes/#031-2019-07-26","text":"This is patch release that fixes compatibility with Solr/Lucene 8.2.","title":"0.3.1 (2019-07-26)"},{"location":"changes/#03-2019-07-25","text":"GitHub Release This release brings some sweeping changes across the codebase, all aimed at making the plugin much simpler to use and less complicated to maintain. However, this also means a lot of breaking changes . It\u2019s best to go through the documentation (which has been simplified and was largely rewritten) again and see what changes you need to apply to your setup. Specifying path resolving is no longer neccessary. You now pass a pointer to one or more files (or regions thereof) directly in the index document. The pointer will be stored with the document and used to locate the input file(s) during highlighting. Refer to the documentation for more details. This should also increase indexing performance and decrease the memory requirements , since at no point does the complete OCR document need to be held in memory. hl.weightMatches now works with UTF8 . You no longer need to ASCII-encode your OCR files to be able to use Solr\u2019s superior highlighting approach. Due to the first change, the plugin now takes care of mapping UTF8 byte-offsets to character offsets by itself. This also means all code related to storing byte offsets in payloads is gone. Specifying the OCR format is no longer neccessary. The plugin now offers a single OcrFormatCharFilter that will auto-detect the OCR format used for a given document and select the correct analysis chain. This means that using multiple OCR formats for the same field is now possible! Performance improvements. Some optimizations were done to the way the plugin seeks through the OCR files. You should see a substantial performance improvement for documents with a low density of multi-byte codepoints, especially English. Also included is a new hl.ocr.maxPassages parameter to control how many passages are looked at for building the response, which can have an enormous impact on performance. Major Breaking Changes : HighlightComponent is now called OcrHighlightComponent for more clarity OCR fields to be highlighted now need to be passed with the hl.ocr.fl parameter Auto-detection of highlightable fields is no longer possible with the standard highlighter, fields to be highlighted need to be passed explicitely with the hl.fl parameter In the order of components, the OCR highlighting component needs to come before the standard highlighter to avoid conflicts.","title":"0.3 (2019-07-25)"},{"location":"changes/#02-2019-07-16","text":"GitHub Release Breaking Change : ALTO and hOCR now have custom CharFilter implementations that should be used instead of HTMLStripCharFilterFactory . Refer to the documentation for more details. Feature: Resolve Hyphenation at indexing time for all supported formats. If a word is broken across multiple lines, it will be indexed as the dehyphenated form. During highlighting, the parts on both lines will be highlighted appropriately. Fix calculation of passages with matches spanning multiple lines, in previous versions some passages would be too small Fix hl.fl parameter handling, a bug in 0.1 made this parameter not have any effect","title":"0.2 (2019-07-16)"},{"location":"changes/#01-2019-06-06","text":"GitHub Release Initial Release","title":"0.1 (2019-06-06)"},{"location":"example/","text":"The repository includes a full-fledged example setup based on the Google Books 1000 and the BNL L\u2019Union Newspaper datasets. The Google Books dataset consists of 1000 Volumes along with their OCRed text in the hOCR format and all book pages as full resolution JPEG images. The BNL dataset consists of 2712 newspaper issues in the ALTO format and all pages as high resolution TIF images. The example ships with a search interface that allows querying the OCRed texts and displays the matching passages as highlighted image and text snippets. We also include a small IIIF-Viewer that allows viewing the documents and searching for text within them. Online version A public instance of this example is available at https://ocrhl.jbaiter.de . The Solr server can be queried at https://ocrhl.jbaiter.de/solr/ocr/select , e.g. for q=\"mason dixon\"~10\" Prerequisites To run the example setup yourself, you will need: Docker and docker-compose Python 3 ~15 GiB of free storage Running the example cd example docker-compose up -d ./ingest.py Access http://localhost:8181 in your browser Search Frontend IIIF Content Search Solr Configuration Walkthrough solrconfig.xml <config> <luceneMatchVersion>7.6</luceneMatchVersion> <directoryFactory name=\"DirectoryFactory\" class=\"${solr.directoryFactory:solr.StandardDirectoryFactory}\"/> <schemaFactory class=\"ClassicIndexSchemaFactory\"/> <!-- Load the plugin JAR from the contrib directory --> <lib dir=\"../../../contrib/ocrsearch/lib\" regex=\".*\\.jar\" /> <!-- Define a search component that takes care of OCR highlighting --> <searchComponent class=\"de.digitalcollections.solrocr.solr.OcrHighlightComponent\" name=\"ocrHighlight\" /> <!-- Add the OCR Highlighting component to the request handler --> <requestHandler name=\"/select\" class=\"solr.SearchHandler\"> <arr name=\"components\"> <str>query</str> <!-- Note that the OCR highlighting component comes **before** the default highlighting component! --> <str>ocrHighlight</str> <str>highlight</str> </arr> </requestHandler> </config> schema.xml <fieldtype name=\"text_ocr\" class=\"solr.TextField\" storeOffsetsWithPositions=\"true\" termVectors=\"true\"> <analyzer type=\"index\"> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.ExternalUtf8ContentFilterFactory\" /> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.OcrCharFilterFactory\" /> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"solr.LowerCaseFilterFactory\"/> <filter class=\"solr.StopFilterFactory\"/> <filter class=\"solr.PorterStemFilterFactory\"/> </analyzer> <analyzer type=\"query\"> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"solr.LowerCaseFilterFactory\"/> <filter class=\"solr.StopFilterFactory\"/> <filter class=\"solr.PorterStemFilterFactory\"/> </analyzer> </fieldtype>","title":"Example Setup"},{"location":"example/#online-version","text":"A public instance of this example is available at https://ocrhl.jbaiter.de . The Solr server can be queried at https://ocrhl.jbaiter.de/solr/ocr/select , e.g. for q=\"mason dixon\"~10\"","title":"Online version"},{"location":"example/#prerequisites","text":"To run the example setup yourself, you will need: Docker and docker-compose Python 3 ~15 GiB of free storage","title":"Prerequisites"},{"location":"example/#running-the-example","text":"cd example docker-compose up -d ./ingest.py Access http://localhost:8181 in your browser","title":"Running the example"},{"location":"example/#search-frontend","text":"","title":"Search Frontend"},{"location":"example/#iiif-content-search","text":"","title":"IIIF Content Search"},{"location":"example/#solr-configuration-walkthrough","text":"solrconfig.xml <config> <luceneMatchVersion>7.6</luceneMatchVersion> <directoryFactory name=\"DirectoryFactory\" class=\"${solr.directoryFactory:solr.StandardDirectoryFactory}\"/> <schemaFactory class=\"ClassicIndexSchemaFactory\"/> <!-- Load the plugin JAR from the contrib directory --> <lib dir=\"../../../contrib/ocrsearch/lib\" regex=\".*\\.jar\" /> <!-- Define a search component that takes care of OCR highlighting --> <searchComponent class=\"de.digitalcollections.solrocr.solr.OcrHighlightComponent\" name=\"ocrHighlight\" /> <!-- Add the OCR Highlighting component to the request handler --> <requestHandler name=\"/select\" class=\"solr.SearchHandler\"> <arr name=\"components\"> <str>query</str> <!-- Note that the OCR highlighting component comes **before** the default highlighting component! --> <str>ocrHighlight</str> <str>highlight</str> </arr> </requestHandler> </config> schema.xml <fieldtype name=\"text_ocr\" class=\"solr.TextField\" storeOffsetsWithPositions=\"true\" termVectors=\"true\"> <analyzer type=\"index\"> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.ExternalUtf8ContentFilterFactory\" /> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.OcrCharFilterFactory\" /> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"solr.LowerCaseFilterFactory\"/> <filter class=\"solr.StopFilterFactory\"/> <filter class=\"solr.PorterStemFilterFactory\"/> </analyzer> <analyzer type=\"query\"> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"solr.LowerCaseFilterFactory\"/> <filter class=\"solr.StopFilterFactory\"/> <filter class=\"solr.PorterStemFilterFactory\"/> </analyzer> </fieldtype>","title":"Solr Configuration Walkthrough"},{"location":"formats/","text":"In general the plugin assumes that all OCR formats encode their documents in a hierarchy of blocks . For all supported formats, we map their block types to these general types: Page : optional if there is only a single page in a document Block : optional if hl.ocr.limitBlock is set to a different value at query time Section : optional Paragraph : optional Line : (optional if hl.ocr.contextBlock is set to a different value at query time) Word : required These block types can be used in the hl.ocr.limitBlock and hl.ocr.contextBlock query parameters to control how the plugin generates snippets. hOCR Block type mapping: Block hOCR class notes Word ocrx_word needs to have a bbox attribute with the coordinates on the page Page ocr_page needs to have a page identifier, either in id attribute or in the ppageno or x_source entry in the title attribute Block ocr_carea / ocrx_block Section ocr_chapter / ocr_section / ocr_subsection / ocr_subsubsection Paragraph ocr_par Line ocr_line or ocrx_line ALTO Caution The coordinates returned by the plugin are not always pixel values , since ALTO supports a variety of different reference units for the coordinates. Check the <MeasurementUnit> value in your ALTO files, if its value is anything other than pixel , you will have to do some additional calculations on the client side to convert to pixel coordinates. Block type mapping: Block ALTO tag notes Word <String /> needs to have CONTENT , HPOS , VPOS , WIDTH and HEIGHT attributes Line <TextLine /> Block <TextBlock /> Page <Page /> needs to have an ID attribute with a page identifier Section not mapped Paragraph not mapped MiniOCR This plugin also includes support for a custom non-standard OCR format that we dubbed MiniOCR , designed to be very simple (and thus performant) to parse and to occupy the least space possible. You should use this format when: you want to store the OCR in the index (to keep the index size as low) reusing the existing OCR files is not possible or practical (to keep occupied disk space low) you want the best possible performance, highlighting MiniOCR is ~25% faster than ALTO and ~50% faster than hOCR A basic example looks like this: <ocr> <p xml:id=\"page_identifier\"> <b> <l><w x=\"50 50 100 100\">A</w> <w x=\"150 50 100 100\">Line</w></l> </b> </p> </ocr> Alternatives for words can be encoded with the \u21ff ( U+21FF ) marker. For example, this is how you would encode a word with the default form clistrias and two alternatives christmas and christrias : <w x=\"50 50 100 100\">clistrias\u21ffchristmas\u21ffchristrias</w> Block type mapping: Block MiniOCR tag notes Word <w/> needs to have box attribute with {x} {y} {width} {height} . Values can be integers or floats between 0 and 1, with the leading 0. omitted Line <l/> Block <b/> Page <p/> needs to have an xml:id attribute with a page identifier. Optionally can have a wh attribute with the {width} {height} values for the page Section not mapped Paragraph not mapped","title":"Supported Formats"},{"location":"formats/#hocr","text":"Block type mapping: Block hOCR class notes Word ocrx_word needs to have a bbox attribute with the coordinates on the page Page ocr_page needs to have a page identifier, either in id attribute or in the ppageno or x_source entry in the title attribute Block ocr_carea / ocrx_block Section ocr_chapter / ocr_section / ocr_subsection / ocr_subsubsection Paragraph ocr_par Line ocr_line or ocrx_line","title":"hOCR"},{"location":"formats/#alto","text":"Caution The coordinates returned by the plugin are not always pixel values , since ALTO supports a variety of different reference units for the coordinates. Check the <MeasurementUnit> value in your ALTO files, if its value is anything other than pixel , you will have to do some additional calculations on the client side to convert to pixel coordinates. Block type mapping: Block ALTO tag notes Word <String /> needs to have CONTENT , HPOS , VPOS , WIDTH and HEIGHT attributes Line <TextLine /> Block <TextBlock /> Page <Page /> needs to have an ID attribute with a page identifier Section not mapped Paragraph not mapped","title":"ALTO"},{"location":"formats/#miniocr","text":"This plugin also includes support for a custom non-standard OCR format that we dubbed MiniOCR , designed to be very simple (and thus performant) to parse and to occupy the least space possible. You should use this format when: you want to store the OCR in the index (to keep the index size as low) reusing the existing OCR files is not possible or practical (to keep occupied disk space low) you want the best possible performance, highlighting MiniOCR is ~25% faster than ALTO and ~50% faster than hOCR A basic example looks like this: <ocr> <p xml:id=\"page_identifier\"> <b> <l><w x=\"50 50 100 100\">A</w> <w x=\"150 50 100 100\">Line</w></l> </b> </p> </ocr> Alternatives for words can be encoded with the \u21ff ( U+21FF ) marker. For example, this is how you would encode a word with the default form clistrias and two alternatives christmas and christrias : <w x=\"50 50 100 100\">clistrias\u21ffchristmas\u21ffchristrias</w> Block type mapping: Block MiniOCR tag notes Word <w/> needs to have box attribute with {x} {y} {width} {height} . Values can be integers or floats between 0 and 1, with the leading 0. omitted Line <l/> Block <b/> Page <p/> needs to have an xml:id attribute with a page identifier. Optionally can have a wh attribute with the {width} {height} values for the page Section not mapped Paragraph not mapped","title":"MiniOCR"},{"location":"indexing/","text":"Indexing OCR documents If you want to store the OCR in the index itself you can skip this section: Just put the OCR content in the field and submit it to Solr for indexing. We recommend using the space-efficient MiniOCR format if you decide to go this way. Indexing OCR documents without storing the actual content in the index is also relatively simple: When building the index document, instead of putting the actual OCR content into the field, you use a source pointer . This pointer will tell the plugin from which location to load the OCR content during indexing and highlighting. The advantage of this approach is a significant reduction in the amount of memory required for both the client and the Solr server, since neither of them has to keep the (potentially very large) OCR document in memory at any time. The client just has a very short pointer, and the plugin will load the contents lazily . Additionally, the index size is kept comparatively small, since Solr only needs to store the locations of the contents and not the (again, potentially very large) contents themselves in the index. Performance When using external files for highlighting, the performance depends to a large degree on how fast the underlying storage is able to perform random I/O. This is why we highly recommend using flash storage for the documents . Another option to increase highlighting performance is to switch from UTF8 to ASCII (with XML-escaped Unicode codepoints) for the encoding of the OCR files. This requires less CPU during decoding, since we don\u2019t have to take multi-byte sequences into account. To signal to the plugin that a given source path is encoded in ASCII, include the {ascii} string after the path, e.g. /mnt/data/ocrdoc.xml{ascii}[31337:41337] . The structure of the source pointers depends on how your actual OCR files on disk map to documents in the Solr index. Encoding The files pointed at by the source pointers need to be UTF-8 or ASCII encoded . Other encodings will lead to unexpected errors and weird behaviour, so make sure the files are in the correct encoding before you index them. One file per Solr document ( 1:1 ) This is the simplest case: The contents of the OCR field in a Solr document correspond exactly to the contents of a single OCR file on disk. The source pointer is simply the path to the file : POST http://solrhost:8983/solr/corename/update { \"id\": \"ocrdoc-1\", \"ocr_text\": \"/mnt/data/ocrdoc-1.xml\" } That\u2019s it, during indexing and highlighting Solr will use the OCR file at /mnt/data/ocrdoc-1.xml to get the text for indexing and highlighting. Multiple files per Solr document ( n:1 ) Frequently the OCR text for a single document (e.g. a book) is stored in one file per page in the document. So, if we want our search results to be these documents and not individual pages from them, we need to instruct the plugin to load the OCR content from multiple files. In this case, the source pointer is the list of all file paths, joined with the + character : POST http://solrhost:8983/solr/corename/update { \"id\": \"ocrdoc-1\", \"ocr_text\": \"/mnt/data/ocrdoc-1_1.xml+/mnt/data/ocrdoc-1_2.xml+/mnt/data/ocrdoc-1_3.xml } For indexing and highlighting, Solr will load the contents of the ocrdoc-1_1.xml , ocrdoc-1_2.xml and ocrdoc-1_2.xml as a single continuous text. Advanced: One or more partial files per Solr document A more complicated situation arises if the Solr documents need to refer to parts of one or more files on disk. This happens for example when you have scans of bound newspaper volumes, which frequently consist of more than 1000 pages. For search purposes, you want to map single articles from issues in this newspaper volume to single Solr documents. If the volume OCR is stored as one file per page, these articles can span multiple files, often in a non-contiguous manner, so you need to refer to regions of these files. For these cases, the source pointer can include one or more byte regions per file : POST http://solrhost:8983/solr/corename/update [ { \"id\": \"article_1863-03-14_8\", \"title\": \"Contiguous article on a single page\", \"ocr_text\": \"/mnt/data/1863_185[216343:331347]\" }, { \"id\": \"article_1863-03-15_12\", \"title\": \"Article spanning from the end of one file to the first part of a second file\", \"ocr_text\": \"/mnt/data/1863_187.xml[76306:]+/mnt/data/1863_188.xml[:196896]\" }, { \"id\": \"article_1863-03-16_2\", \"title\": \"Article split between two pages, with the content on the first page split by an advertisement.\", \"ocr_text\": \"/mnt/data/1863_191.xml[1578:8937,12478:17621]+/mnt/data/1863_192.xml[837:28432]\" } ] As before, we concatenate multiple file paths with the + character. The source regions for each file are listed as comma-separated byte-regions inside of square brackets. The format of the regions is inspired by Python\u2019s slicing syntax and can take these forms: start: \u2192 Everything from byte offset start to the end of the file start:end \u2192 Everything between the byte offsets start (inclusive) and end (exclusive) :end \u2192 Everything from the start of the file to byte offset end (exclusive) Region Requirements\u201d The concatenated content of your regions must be a half-way valid XML structure. While we tolerate unclosed tags or unmatched closing tags (they often can\u2019t be avoided), other errors such as partial tags (i.e. a missing < or > ) will lead to an error during indexing. To get correct page numbers in your responses, make sure that you include any and all page openings for your content in the set of regions. For example, if your document is an article that spans from the bottom of one page to the top of the next, you will have to include a region for the opening element of the first page so we can determine the page for the first part of the article during highlighting Byte Offsets The region offsets are expected as byte offsets . Take care that the start and end of each region fall on the start of a valid unicode byte sequence, and not in the middle of a multi-byte sequence. Care needs to be taken when determining the offsets, since obtaining byte offsets for UTF8-encoded text files is difficult in some programming languages (most notoriously Java, use the net.byteseek:byteseek package) Example Implementation The example setup on GitHub uses a Python script to index articles from multi-page newspaper scans into Solr. It works by first extracting the OCR block ids for each article from a METS file and then finds the byte regions these OCR blocks are located in to build the source pointer for each article.","title":"Indexing"},{"location":"indexing/#indexing-ocr-documents","text":"If you want to store the OCR in the index itself you can skip this section: Just put the OCR content in the field and submit it to Solr for indexing. We recommend using the space-efficient MiniOCR format if you decide to go this way. Indexing OCR documents without storing the actual content in the index is also relatively simple: When building the index document, instead of putting the actual OCR content into the field, you use a source pointer . This pointer will tell the plugin from which location to load the OCR content during indexing and highlighting. The advantage of this approach is a significant reduction in the amount of memory required for both the client and the Solr server, since neither of them has to keep the (potentially very large) OCR document in memory at any time. The client just has a very short pointer, and the plugin will load the contents lazily . Additionally, the index size is kept comparatively small, since Solr only needs to store the locations of the contents and not the (again, potentially very large) contents themselves in the index. Performance When using external files for highlighting, the performance depends to a large degree on how fast the underlying storage is able to perform random I/O. This is why we highly recommend using flash storage for the documents . Another option to increase highlighting performance is to switch from UTF8 to ASCII (with XML-escaped Unicode codepoints) for the encoding of the OCR files. This requires less CPU during decoding, since we don\u2019t have to take multi-byte sequences into account. To signal to the plugin that a given source path is encoded in ASCII, include the {ascii} string after the path, e.g. /mnt/data/ocrdoc.xml{ascii}[31337:41337] . The structure of the source pointers depends on how your actual OCR files on disk map to documents in the Solr index. Encoding The files pointed at by the source pointers need to be UTF-8 or ASCII encoded . Other encodings will lead to unexpected errors and weird behaviour, so make sure the files are in the correct encoding before you index them.","title":"Indexing OCR documents"},{"location":"indexing/#one-file-per-solr-document-11","text":"This is the simplest case: The contents of the OCR field in a Solr document correspond exactly to the contents of a single OCR file on disk. The source pointer is simply the path to the file : POST http://solrhost:8983/solr/corename/update { \"id\": \"ocrdoc-1\", \"ocr_text\": \"/mnt/data/ocrdoc-1.xml\" } That\u2019s it, during indexing and highlighting Solr will use the OCR file at /mnt/data/ocrdoc-1.xml to get the text for indexing and highlighting.","title":"One file per Solr document (1:1)"},{"location":"indexing/#multiple-files-per-solr-document-n1","text":"Frequently the OCR text for a single document (e.g. a book) is stored in one file per page in the document. So, if we want our search results to be these documents and not individual pages from them, we need to instruct the plugin to load the OCR content from multiple files. In this case, the source pointer is the list of all file paths, joined with the + character : POST http://solrhost:8983/solr/corename/update { \"id\": \"ocrdoc-1\", \"ocr_text\": \"/mnt/data/ocrdoc-1_1.xml+/mnt/data/ocrdoc-1_2.xml+/mnt/data/ocrdoc-1_3.xml } For indexing and highlighting, Solr will load the contents of the ocrdoc-1_1.xml , ocrdoc-1_2.xml and ocrdoc-1_2.xml as a single continuous text.","title":"Multiple files per Solr document (n:1)"},{"location":"indexing/#advanced-one-or-more-partial-files-per-solr-document","text":"A more complicated situation arises if the Solr documents need to refer to parts of one or more files on disk. This happens for example when you have scans of bound newspaper volumes, which frequently consist of more than 1000 pages. For search purposes, you want to map single articles from issues in this newspaper volume to single Solr documents. If the volume OCR is stored as one file per page, these articles can span multiple files, often in a non-contiguous manner, so you need to refer to regions of these files. For these cases, the source pointer can include one or more byte regions per file : POST http://solrhost:8983/solr/corename/update [ { \"id\": \"article_1863-03-14_8\", \"title\": \"Contiguous article on a single page\", \"ocr_text\": \"/mnt/data/1863_185[216343:331347]\" }, { \"id\": \"article_1863-03-15_12\", \"title\": \"Article spanning from the end of one file to the first part of a second file\", \"ocr_text\": \"/mnt/data/1863_187.xml[76306:]+/mnt/data/1863_188.xml[:196896]\" }, { \"id\": \"article_1863-03-16_2\", \"title\": \"Article split between two pages, with the content on the first page split by an advertisement.\", \"ocr_text\": \"/mnt/data/1863_191.xml[1578:8937,12478:17621]+/mnt/data/1863_192.xml[837:28432]\" } ] As before, we concatenate multiple file paths with the + character. The source regions for each file are listed as comma-separated byte-regions inside of square brackets. The format of the regions is inspired by Python\u2019s slicing syntax and can take these forms: start: \u2192 Everything from byte offset start to the end of the file start:end \u2192 Everything between the byte offsets start (inclusive) and end (exclusive) :end \u2192 Everything from the start of the file to byte offset end (exclusive) Region Requirements\u201d The concatenated content of your regions must be a half-way valid XML structure. While we tolerate unclosed tags or unmatched closing tags (they often can\u2019t be avoided), other errors such as partial tags (i.e. a missing < or > ) will lead to an error during indexing. To get correct page numbers in your responses, make sure that you include any and all page openings for your content in the set of regions. For example, if your document is an article that spans from the bottom of one page to the top of the next, you will have to include a region for the opening element of the first page so we can determine the page for the first part of the article during highlighting Byte Offsets The region offsets are expected as byte offsets . Take care that the start and end of each region fall on the start of a valid unicode byte sequence, and not in the middle of a multi-byte sequence. Care needs to be taken when determining the offsets, since obtaining byte offsets for UTF8-encoded text files is difficult in some programming languages (most notoriously Java, use the net.byteseek:byteseek package) Example Implementation The example setup on GitHub uses a Python script to index articles from multi-page newspaper scans into Solr. It works by first extracting the OCR block ids for each article from a METS file and then finds the byte regions these OCR blocks are located in to build the source pointer for each article.","title":"Advanced: One or more partial files per Solr document"},{"location":"installation/","text":"Requirements Some familiarity with configuring Solr Solr >= 7.5 OCR documents need to be in hOCR , ALTO or MiniOCR formats, with at least page-, and word-level segmentation Obtaining the plugin JAR To use the latest release version, refer to the GitHub Releases list . From there, download the JAR file for the latest version. To make the plugin available to Solr, create a new directory $SOLR_HOME/contrib/ocrsearch/lib and place the JAR you just downloaded there. Core Configuration To enable the use of the plugin for your Solr core, you will have to edit both the solrconfig.xml and the schema.xml file in your core\u2019s conf directory. SolrConfig In your core\u2019s `solrconfig.xml, you need to: Instruct the core to load the OCR highlighting plugin, so it can find the classes needed to perform OCR indexing and highlighting. Define a search component that will perform the OCR highlighting at query time Add the search component to your request handlers that will trigger the highlighting. <config> <!-- ...other configuration options... --> <!-- Tell Solr to load all JAR files from the directory installed the plugin to. This assumes a directory structure where the cores are in `$SOLR_HOME/server/solr/$CORE` and the plugin JAR was installed in `$SOLR_HOME/contrib/ocrsearch/lib`. Adjust the path if you setup differs. --> <lib dir=\"../../../contrib/ocrsearch/lib\" regex=\".*\\.jar\" /> <!-- Add a new named search component that takes care of highlighting OCR field values. --> <searchComponent class=\"de.digitalcollections.solrocr.solr.OcrHighlightComponent\" name=\"ocrHighlight\" /> <!-- ...other search components... --> <!-- Instruct the request handlers you want to enable OCR highlighting for to include the search component you defined above. This example uses the standard /select handler. CAUTION: Make sure that the OCR highlight component is listed **before** the standard highlighting component, but **after** the query component. --> <requestHandler name=\"/select\" class=\"solr.SearchHandler\"> <arr name=\"components\"> <str>query</str> <str>ocrHighlight</str> <str>highlight</str> </arr> </requestHandler> </config> If you run into problems, a look into these sections of the Solr user\u2019s guide might be helpful: Resource and Plugin Loading RequestHandlers and SearchComponents in SolrConfig Schema In the core\u2019s schema.xml , you need to: Define a new field type that will hold your indexed OCR text Define which fields are going to hold the indexed OCR text. The field type for OCR text is usually identical to your regular text field, with the difference that there are one or two extra character filters at the beginning of your index analysis chain : - ExternalUtf8ContentFilterFactory will (optionally) allow you to index and highlight OCR from external sources on the file system. More on this in the Indexing chapter . - OcrCharFilterFactory will retrieve the raw OCR data and extract the plain text that is going to pass through the rest of the analysis chain. It will auto-detect the used OCR formats, which means that you can use different OCR formats alongside each other . After this filter, Solr will treat the field just like a regular text field for purposes of analysis. <schema> <types> <fieldtype name=\"text_ocr\" class=\"solr.TextField\" storeOffsetsWithPositions=\"true\" termVectors=\"true\"> <analyzer type=\"index\"> <!-- For loading external files as field values during indexing --> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.ExternalUtf8ContentFilterFactory\" /> <!-- For converting OCR to plaintext --> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.OcrCharFilterFactory\" /> <!-- ...rest of your index analysis chain... --> </analyzer> <analyzer type=\"query\"> <!-- your query analysis chain, should not include the character filters --> </analyzer> </fieldtype> </types> <fields> <!-- ...your other fields ... --> <!-- A field that uses the OCR field type. Has to be `stored`. --> <field name=\"ocr_text\" type=\"text_ocr\" multiValued=\"false\" indexed=\"true\" stored=\"true\" /> </fields> </schema> If you struggle with setting up your schema, a look into the Schema Design chapter of the Solr user\u2019s guide might be helpful. No support for multi-valued fields Due to certain limitations in Lucene/Solr, it is currently not possible to use multi-valued fields for OCR highlighting . You can work around this by leveraging some of the advanced features of source pointers , though.","title":"Installation"},{"location":"installation/#requirements","text":"Some familiarity with configuring Solr Solr >= 7.5 OCR documents need to be in hOCR , ALTO or MiniOCR formats, with at least page-, and word-level segmentation","title":"Requirements"},{"location":"installation/#obtaining-the-plugin-jar","text":"To use the latest release version, refer to the GitHub Releases list . From there, download the JAR file for the latest version. To make the plugin available to Solr, create a new directory $SOLR_HOME/contrib/ocrsearch/lib and place the JAR you just downloaded there.","title":"Obtaining the plugin JAR"},{"location":"installation/#core-configuration","text":"To enable the use of the plugin for your Solr core, you will have to edit both the solrconfig.xml and the schema.xml file in your core\u2019s conf directory.","title":"Core Configuration"},{"location":"installation/#solrconfig","text":"In your core\u2019s `solrconfig.xml, you need to: Instruct the core to load the OCR highlighting plugin, so it can find the classes needed to perform OCR indexing and highlighting. Define a search component that will perform the OCR highlighting at query time Add the search component to your request handlers that will trigger the highlighting. <config> <!-- ...other configuration options... --> <!-- Tell Solr to load all JAR files from the directory installed the plugin to. This assumes a directory structure where the cores are in `$SOLR_HOME/server/solr/$CORE` and the plugin JAR was installed in `$SOLR_HOME/contrib/ocrsearch/lib`. Adjust the path if you setup differs. --> <lib dir=\"../../../contrib/ocrsearch/lib\" regex=\".*\\.jar\" /> <!-- Add a new named search component that takes care of highlighting OCR field values. --> <searchComponent class=\"de.digitalcollections.solrocr.solr.OcrHighlightComponent\" name=\"ocrHighlight\" /> <!-- ...other search components... --> <!-- Instruct the request handlers you want to enable OCR highlighting for to include the search component you defined above. This example uses the standard /select handler. CAUTION: Make sure that the OCR highlight component is listed **before** the standard highlighting component, but **after** the query component. --> <requestHandler name=\"/select\" class=\"solr.SearchHandler\"> <arr name=\"components\"> <str>query</str> <str>ocrHighlight</str> <str>highlight</str> </arr> </requestHandler> </config> If you run into problems, a look into these sections of the Solr user\u2019s guide might be helpful: Resource and Plugin Loading RequestHandlers and SearchComponents in SolrConfig","title":"SolrConfig"},{"location":"installation/#schema","text":"In the core\u2019s schema.xml , you need to: Define a new field type that will hold your indexed OCR text Define which fields are going to hold the indexed OCR text. The field type for OCR text is usually identical to your regular text field, with the difference that there are one or two extra character filters at the beginning of your index analysis chain : - ExternalUtf8ContentFilterFactory will (optionally) allow you to index and highlight OCR from external sources on the file system. More on this in the Indexing chapter . - OcrCharFilterFactory will retrieve the raw OCR data and extract the plain text that is going to pass through the rest of the analysis chain. It will auto-detect the used OCR formats, which means that you can use different OCR formats alongside each other . After this filter, Solr will treat the field just like a regular text field for purposes of analysis. <schema> <types> <fieldtype name=\"text_ocr\" class=\"solr.TextField\" storeOffsetsWithPositions=\"true\" termVectors=\"true\"> <analyzer type=\"index\"> <!-- For loading external files as field values during indexing --> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.ExternalUtf8ContentFilterFactory\" /> <!-- For converting OCR to plaintext --> <charFilter class=\"de.digitalcollections.solrocr.lucene.filters.OcrCharFilterFactory\" /> <!-- ...rest of your index analysis chain... --> </analyzer> <analyzer type=\"query\"> <!-- your query analysis chain, should not include the character filters --> </analyzer> </fieldtype> </types> <fields> <!-- ...your other fields ... --> <!-- A field that uses the OCR field type. Has to be `stored`. --> <field name=\"ocr_text\" type=\"text_ocr\" multiValued=\"false\" indexed=\"true\" stored=\"true\" /> </fields> </schema> If you struggle with setting up your schema, a look into the Schema Design chapter of the Solr user\u2019s guide might be helpful. No support for multi-valued fields Due to certain limitations in Lucene/Solr, it is currently not possible to use multi-valued fields for OCR highlighting . You can work around this by leveraging some of the advanced features of source pointers , though.","title":"Schema"},{"location":"performance/","text":"Performance Considerations Highlighting based on locally stored files can take a long time, depending on the environment. This section gives some hints on potential knobs to tune to improve the performance. Use JDK >= 9 Java 9 introduced a feature called String Compaction that has a significant impact on several hot code paths used during OCR highlighting. You can expect a reduction in runtime of more than 50% if you use a JVM with string compaction enabled compared to one without (assuming you\u2019re running on flash storage). We highly recommend using the latest LTS OpenJDK version released after Java 9, which as of January 2021 is OpenJDK 11. Performance Analysis Before you start tuning the plugin, it is important to spend some time on analyzing the nature of the problems: Check Solr queries with debug=timing : How much of the response time is actually spent in the OCR highlighting component? On the operating system level (if you\u2019re on a Linux system), use BCC Tools , especially {nfs/xfs/ext/...}slower and {nfs/xfs/ext/...}dist to check if the performance issues are due to I/O latency. Storage Layer The plugin spends a lot of time on randomly reading small sections of the target files from disk. This means that the performance characteristics of the underlying storage system have a huge effect on the performance of the plugin. Important factors include: Random Read Latency : What is the time required to seek to a random location and read a small chunk? Number of possible parallel reads (see below): Does the storage layer support more than one active reader? Generally speaking, local storage is better than remote storage (like NFS or CIFS), due to the network latency, and flash-based storage is better than disk-based storage, due to the lower random read latency. A RAID setup is preferred over a JBOD setup, due to the potential for parallel reads. Plugin configuration The plugin offers the possibility to perform a concurrent read-ahead of highlighting target files . This will perform \u201cdummy\u201d reads on multiple parallel threads, with the intent to fill the operating system\u2019s page cache with the contents of the highlighting targets, so that the actual highlighting process is performed on data read from the cache (which resides in main memory). This is mainly useful for storage layers that benefit from parallel reads, since the highlighting process is strongly sequential and performing the read-ahead concurrently can reduce latency. To enable it, add the enablePreload=true attribute on the OCR highlighting component in your core\u2019s solrconfig.xml . It is important to accompany this with benchmarking and monitoring, the available settings should be tuned to the environment: preloadReadSize : Size in bytes of read-ahead block reads, should be aligned with file system block size (or rsize for NFS file systems). Defaults to 32768 . preloadConcurrency : Number of threads to perform read-ahead. Optimal settings have to be determined via experimentation. Defaults to 8 . This approach relies on the OS-level page cache, so make sure you have enough spare RAM available on your machine to actually benefit from this! Use BCC\u2019s *slower tools to verify that it\u2019s a solr-ocrhighlight thread that performs most of the reads and not the actual query thread ( qtp.... ). If you run the same query twice, you shouldn\u2019t see a lot of reads from either the qtp... or solr-ocrhlighight threads on the second run. Example configuration tuned for remote NFS storage mounted with rsize=65536 : <searchComponent class=\"de.digitalcollections.solrocr.solr.OcrHighlightComponent\" name=\"ocrHighlight\" enablePreload=\"true\" preloadReadSize=\"65536\" preloadConcurrency=\"8\"/> Runtime configuration Another option to influence the performance of the plugin is to tune some runtime options for highlighting. For any of these, refer to the Querying section for more details. If you\u2019re storing documents at the page-level in the index, you can set the hl.ocr.trackPages parameter to false (default is true ). This will skip seeking backward in the input from the match position to find the containing page, which can be costly. Tune the number of candidate passages for ranking with hl.ocr.maxPassages , which defaults to 100 . Lowering this is better for performance, but means that the resulting snippets might not be the most relevant in the document. Change the limit ( hl.ocr.limitBlock ) and/or context block types ( hl.ocr.contextBlock ) to something lower in the block hierarchy to reduce the amount of reads in the OCR files. Another knob to tune is the number of context blocks for each hit ( hl.ocr.contextSize ), with the same effect. The last resort if highlighting takes too long is to pass the hl.ocr.timeAllowed parameter, which stops highlighting any further documents if a given timeout is exceeded.","title":"Performance Tuning"},{"location":"performance/#performance-considerations","text":"Highlighting based on locally stored files can take a long time, depending on the environment. This section gives some hints on potential knobs to tune to improve the performance. Use JDK >= 9 Java 9 introduced a feature called String Compaction that has a significant impact on several hot code paths used during OCR highlighting. You can expect a reduction in runtime of more than 50% if you use a JVM with string compaction enabled compared to one without (assuming you\u2019re running on flash storage). We highly recommend using the latest LTS OpenJDK version released after Java 9, which as of January 2021 is OpenJDK 11.","title":"Performance Considerations"},{"location":"performance/#performance-analysis","text":"Before you start tuning the plugin, it is important to spend some time on analyzing the nature of the problems: Check Solr queries with debug=timing : How much of the response time is actually spent in the OCR highlighting component? On the operating system level (if you\u2019re on a Linux system), use BCC Tools , especially {nfs/xfs/ext/...}slower and {nfs/xfs/ext/...}dist to check if the performance issues are due to I/O latency.","title":"Performance Analysis"},{"location":"performance/#storage-layer","text":"The plugin spends a lot of time on randomly reading small sections of the target files from disk. This means that the performance characteristics of the underlying storage system have a huge effect on the performance of the plugin. Important factors include: Random Read Latency : What is the time required to seek to a random location and read a small chunk? Number of possible parallel reads (see below): Does the storage layer support more than one active reader? Generally speaking, local storage is better than remote storage (like NFS or CIFS), due to the network latency, and flash-based storage is better than disk-based storage, due to the lower random read latency. A RAID setup is preferred over a JBOD setup, due to the potential for parallel reads.","title":"Storage Layer"},{"location":"performance/#plugin-configuration","text":"The plugin offers the possibility to perform a concurrent read-ahead of highlighting target files . This will perform \u201cdummy\u201d reads on multiple parallel threads, with the intent to fill the operating system\u2019s page cache with the contents of the highlighting targets, so that the actual highlighting process is performed on data read from the cache (which resides in main memory). This is mainly useful for storage layers that benefit from parallel reads, since the highlighting process is strongly sequential and performing the read-ahead concurrently can reduce latency. To enable it, add the enablePreload=true attribute on the OCR highlighting component in your core\u2019s solrconfig.xml . It is important to accompany this with benchmarking and monitoring, the available settings should be tuned to the environment: preloadReadSize : Size in bytes of read-ahead block reads, should be aligned with file system block size (or rsize for NFS file systems). Defaults to 32768 . preloadConcurrency : Number of threads to perform read-ahead. Optimal settings have to be determined via experimentation. Defaults to 8 . This approach relies on the OS-level page cache, so make sure you have enough spare RAM available on your machine to actually benefit from this! Use BCC\u2019s *slower tools to verify that it\u2019s a solr-ocrhighlight thread that performs most of the reads and not the actual query thread ( qtp.... ). If you run the same query twice, you shouldn\u2019t see a lot of reads from either the qtp... or solr-ocrhlighight threads on the second run. Example configuration tuned for remote NFS storage mounted with rsize=65536 : <searchComponent class=\"de.digitalcollections.solrocr.solr.OcrHighlightComponent\" name=\"ocrHighlight\" enablePreload=\"true\" preloadReadSize=\"65536\" preloadConcurrency=\"8\"/>","title":"Plugin configuration"},{"location":"performance/#runtime-configuration","text":"Another option to influence the performance of the plugin is to tune some runtime options for highlighting. For any of these, refer to the Querying section for more details. If you\u2019re storing documents at the page-level in the index, you can set the hl.ocr.trackPages parameter to false (default is true ). This will skip seeking backward in the input from the match position to find the containing page, which can be costly. Tune the number of candidate passages for ranking with hl.ocr.maxPassages , which defaults to 100 . Lowering this is better for performance, but means that the resulting snippets might not be the most relevant in the document. Change the limit ( hl.ocr.limitBlock ) and/or context block types ( hl.ocr.contextBlock ) to something lower in the block hierarchy to reduce the amount of reads in the OCR files. Another knob to tune is the number of context blocks for each hit ( hl.ocr.contextSize ), with the same effect. The last resort if highlighting takes too long is to pass the hl.ocr.timeAllowed parameter, which stops highlighting any further documents if a given timeout is exceeded.","title":"Runtime configuration"},{"location":"query/","text":"Enabling Highlighting for OCR Fields To enable highlighting, make sure you set hl=true in you query. Additionally, you need to pass the OCR fields that you want to have highlighted in the hl.ocr.fl parameter. Response Format With OCR highlighting enabled, your Solr response will now include a new item ocrHighlighting , mapping all highlighted OCR fields to their highlighting snippets: GET https://ocrhl.jbaiter.de/solr/ocr/select?q=ocr_text:%22Mason%20Dixon%22~10&hl=true&hl.ocr.fl=ocr_text { \"responseHeader\": { \"status\": 0, \"QTime\": 111, }, \"response\": { \"numFound\": 1, \"start\": 0, \"docs\": [{ \"id\": \"doc_id\" }] }, \"ocrHighlighting\": { \"ident\": { \"ocr_text\": { \"numTotal\": 4, \"snippets\": [/* see below */] } } } } The numTotal key will specify how many highlighting snippets were found in the document for the query. How many of these are actually contained in the response under the snippets key depends on the value for the hl.snippets parameter, which defaults to 1 . The objects contained under the snippets key are structured like this: { \"text\": \"to those parts, subject to unreasonable claims from the proprietor \" \"of Maryland, until the year 17C2, when the whole controversy was \" \"settled by Charles <em>Mason and Jeremiah Dixon</em>, upon their \" \"return from an observation of the transit of Venus, at the Cape of \" \"Good Hope, where they\", \"score\": 5555104.5, \"pages\": [ { \"id\": \"page_380\", \"width\": 1436, \"height\": 2427 } ], \"regions\": [ { \"ulx\": 196, \"uly\": 1703, \"lrx\": 1232, \"lry\": 1968, \"pageIdx\": 0 } ], \"highlights\":[ [{ \"text\": \"Mason and Jeremiah\", \"ulx\": 675, \"uly\": 110, \"lrx\": 1036, \"lry\": 145, \"parentRegionIdx\": 0}, { \"text\": \"Dixon,\", \"ulx\": 1, \"uly\": 167, \"lrx\": 119, \"lry\": 204, \"parentRegionIdx\": 0 }] ] } text contains the plain text of the region containing one or more matches. The matches themselves are wrapped in <em> tags (this can be customized with the hl.tag.pre and hl.tag.post parameters). score contains the numerical score (relative to the whole document) that Solr assigned to the snippet pages contains a list of pages the snippet appears on along with their pixel dimensions. This can be useful for rendering highlights, e.g. if the highlighting target image is scaled down from the source image. regions contains a list of regions that the snippet is located on. Usually this will contain only one item, but in cases where a phrase spans multiple pages, it will contain a region for every page involved in the match. The object includes coordinates for all four corners it is defined by, as well as the identifier of the page the region is located on. highlights contains a list of regions that contain the actual matches for the query as well as the text that matched the query and the page the match occurred on. Note that the coordinates are relative to the containing region, not the page (this can be changed with the hl.ocr.absoluteHighlights parameter). To find the corresponding region for a match, use the regionIdx value, which refers to the index in the regions array that the surrounding region is located at. Generation of Snippet Regions To determine how to build regions from a set of matches, the plugin takes into account the structure of the OCR document surrounding each match. Consider the following page from a document, with two matches on two adjacent lines. First, the plugin will determine the context of each match, starting with the first match. For this, it searches for a fixed number of context blocks before and after the block it is in. The type of context block is configured with the hl.ocr.contextBlock parameter and it defaults to line (for a list of all possible block types, refer to the full list of parameters). The number of context blocks to use for the snippet context is set with the hl.ocr.contextSize parameter and defaults to 2 . So for our first match, the context will be the two lines above and below the line of the match. It then proceeds to the next match, which happens to be located on the next line block. If the new match is part of the previous match\u2019s context, it will be merged into the previous snippet and the snippet will be updated. In the example, the snippet would grow by two lines (since the context is two line blocks below the last match). However, the plugin will not expand the context beyond the limits of certain blocks. In our example, we decided that we don\u2019t want contexts to cross block elements. This limit block can be configured with the hl.ocr.limitBlock parameter and defaults to block (which is format-dependent, but usually refers to a group of one or more paragraphs ). If, during context building, a block is encountered that is of this type (or an even higher level), the context building will be stopped. In the example, the context of the snippet is only one line below the last match, since this is the last line in the containing block . Note that you will still occasionally get snippets that span multiple blocks , in the case when you searched for a phrase and the phrase match crosses multiple blocks. Available highlighting parameters Since the OCR highlighter builds upon Solr\u2019s UnifiedHighlighter , you can use most of the Common Highlighter parameters and the Unified Highlighter parameters . Of special interest for purposes of OCR highlighting are these: hl Enable highlighting with on . Required if you want highlighting of any kind. Defaults to off . hl.snippets Number of snippets per document to include in the response, defaults to 1 . hl.tag.pre / hl.tag.post Strings to wrap matches in the plaintext version of the snippet with, defaults to <em> / </em> . hl.weightMatches Uses a new and improved highlighting algorithm that is much more precise than the old approach. For example, with this set to true , results for a phrase query \"foo bar baz\" will actually be highlighted as <em>foo bar baz</em> and not as <em>foo</em> <em>bar</em> <em>baz</em> . Defaults to off in Solr versions < 8.0, on for all versions > 8.0. Additionally, the plugin allows you to customize various OCR-specific parameters: hl.ocr.fl Fields to enable OCR highlighting for. This is required if you want to have OCR highlighting. hl.ocr.contextBlock Select which block type should be considered for determining the context. Valid values are word , line , paragraph , block or page and defaults to line . hl.ocr.contextSize Set the number of blocks above and below the matching block to be included in the passage. Defaults to 2 . hl.ocr.limitBlock Set the block type that the passage context may not exceed. Valid values are none word , line , paragraph , block or page . This value defaults to block . hl.ocr.alignSpans Ensure that the spans in the highlighted text match the text of the highlighted image parts exactly. By default ( false ), text spans will be more precise than image spans, since they can be defined at the character-level, while image spans can only be as precise as the word boundaries in the OCR. hl.ocr.pageId : Only show passages from the page with this identifier. Useful in combination with a fq for a specific document if you want to implement a \u201cSearch on this page\u201d feature (e.g. for the IIIF Content Search API ). hl.ocr.absoluteHighlights : When on , return the coordinates of highlighted regions as absolute coordinates, i.e. relative to the containing page, not the containing snippet region. Defaults to off . hl.ocr.maxPassages : Only score and rank at most this many passages to get the final list of snippets. This parameter can be used to tweak the performance. If you notice that highlighting takes a long time, you might consider lowering this value. The default is 100 . The practical result is that only the first n matches in a document will be considered for highlighting, i.e. if a more relevant passage occurs at the end of the document, it is more likely to not be considered if the total number of matches in the document exceeds this number. hl.ocr.timeAllowed : Due to the fact that generating highlighting snippets from disk can take a very long time, depending on the number of documents and snippets, you can limit the time OCR highlighting should take. The parameter takes the maximum allowed time in milliseconds . If the timeout is exceeded during highlighting, the document currently being highlighted and any other remaining documents will be skipped. The highlighting response will then only include snippets from documents that were highlighted before the timeout. The presence of partial results will be indicated by the partialOcrHighlights key in the responseHeader . hl.ocr.trackPages : When off (defaults to on ), you will not get information the containing page of a given snippet. This can improve highlighting performance, since less of the input file needs to be read and should be disabled when you index your documents at the page-level, i.e. when the identify of the page is encoded elsewhere in the document. hl.ocr.scorePassages : When off (defaults to on ), the snippets are returned in order of their occurrence in the document. Otherwise, it will follow Solr\u2019s default strategy for scoring highlighting snippets, which treats each candidate snippet as a \u2018mini-document\u2019 that is scored using TF-IDF/BM25, treating the parent document as the corpus. This results in a relevance score in relation to the parent document, i.e. the first snippet should be the most relevant snippet in the document.","title":"Querying"},{"location":"query/#enabling-highlighting-for-ocr-fields","text":"To enable highlighting, make sure you set hl=true in you query. Additionally, you need to pass the OCR fields that you want to have highlighted in the hl.ocr.fl parameter.","title":"Enabling Highlighting for OCR Fields"},{"location":"query/#response-format","text":"With OCR highlighting enabled, your Solr response will now include a new item ocrHighlighting , mapping all highlighted OCR fields to their highlighting snippets: GET https://ocrhl.jbaiter.de/solr/ocr/select?q=ocr_text:%22Mason%20Dixon%22~10&hl=true&hl.ocr.fl=ocr_text { \"responseHeader\": { \"status\": 0, \"QTime\": 111, }, \"response\": { \"numFound\": 1, \"start\": 0, \"docs\": [{ \"id\": \"doc_id\" }] }, \"ocrHighlighting\": { \"ident\": { \"ocr_text\": { \"numTotal\": 4, \"snippets\": [/* see below */] } } } } The numTotal key will specify how many highlighting snippets were found in the document for the query. How many of these are actually contained in the response under the snippets key depends on the value for the hl.snippets parameter, which defaults to 1 . The objects contained under the snippets key are structured like this: { \"text\": \"to those parts, subject to unreasonable claims from the proprietor \" \"of Maryland, until the year 17C2, when the whole controversy was \" \"settled by Charles <em>Mason and Jeremiah Dixon</em>, upon their \" \"return from an observation of the transit of Venus, at the Cape of \" \"Good Hope, where they\", \"score\": 5555104.5, \"pages\": [ { \"id\": \"page_380\", \"width\": 1436, \"height\": 2427 } ], \"regions\": [ { \"ulx\": 196, \"uly\": 1703, \"lrx\": 1232, \"lry\": 1968, \"pageIdx\": 0 } ], \"highlights\":[ [{ \"text\": \"Mason and Jeremiah\", \"ulx\": 675, \"uly\": 110, \"lrx\": 1036, \"lry\": 145, \"parentRegionIdx\": 0}, { \"text\": \"Dixon,\", \"ulx\": 1, \"uly\": 167, \"lrx\": 119, \"lry\": 204, \"parentRegionIdx\": 0 }] ] } text contains the plain text of the region containing one or more matches. The matches themselves are wrapped in <em> tags (this can be customized with the hl.tag.pre and hl.tag.post parameters). score contains the numerical score (relative to the whole document) that Solr assigned to the snippet pages contains a list of pages the snippet appears on along with their pixel dimensions. This can be useful for rendering highlights, e.g. if the highlighting target image is scaled down from the source image. regions contains a list of regions that the snippet is located on. Usually this will contain only one item, but in cases where a phrase spans multiple pages, it will contain a region for every page involved in the match. The object includes coordinates for all four corners it is defined by, as well as the identifier of the page the region is located on. highlights contains a list of regions that contain the actual matches for the query as well as the text that matched the query and the page the match occurred on. Note that the coordinates are relative to the containing region, not the page (this can be changed with the hl.ocr.absoluteHighlights parameter). To find the corresponding region for a match, use the regionIdx value, which refers to the index in the regions array that the surrounding region is located at.","title":"Response Format"},{"location":"query/#generation-of-snippet-regions","text":"To determine how to build regions from a set of matches, the plugin takes into account the structure of the OCR document surrounding each match. Consider the following page from a document, with two matches on two adjacent lines. First, the plugin will determine the context of each match, starting with the first match. For this, it searches for a fixed number of context blocks before and after the block it is in. The type of context block is configured with the hl.ocr.contextBlock parameter and it defaults to line (for a list of all possible block types, refer to the full list of parameters). The number of context blocks to use for the snippet context is set with the hl.ocr.contextSize parameter and defaults to 2 . So for our first match, the context will be the two lines above and below the line of the match. It then proceeds to the next match, which happens to be located on the next line block. If the new match is part of the previous match\u2019s context, it will be merged into the previous snippet and the snippet will be updated. In the example, the snippet would grow by two lines (since the context is two line blocks below the last match). However, the plugin will not expand the context beyond the limits of certain blocks. In our example, we decided that we don\u2019t want contexts to cross block elements. This limit block can be configured with the hl.ocr.limitBlock parameter and defaults to block (which is format-dependent, but usually refers to a group of one or more paragraphs ). If, during context building, a block is encountered that is of this type (or an even higher level), the context building will be stopped. In the example, the context of the snippet is only one line below the last match, since this is the last line in the containing block . Note that you will still occasionally get snippets that span multiple blocks , in the case when you searched for a phrase and the phrase match crosses multiple blocks.","title":"Generation of Snippet Regions"},{"location":"query/#available-highlighting-parameters","text":"Since the OCR highlighter builds upon Solr\u2019s UnifiedHighlighter , you can use most of the Common Highlighter parameters and the Unified Highlighter parameters . Of special interest for purposes of OCR highlighting are these: hl Enable highlighting with on . Required if you want highlighting of any kind. Defaults to off . hl.snippets Number of snippets per document to include in the response, defaults to 1 . hl.tag.pre / hl.tag.post Strings to wrap matches in the plaintext version of the snippet with, defaults to <em> / </em> . hl.weightMatches Uses a new and improved highlighting algorithm that is much more precise than the old approach. For example, with this set to true , results for a phrase query \"foo bar baz\" will actually be highlighted as <em>foo bar baz</em> and not as <em>foo</em> <em>bar</em> <em>baz</em> . Defaults to off in Solr versions < 8.0, on for all versions > 8.0. Additionally, the plugin allows you to customize various OCR-specific parameters: hl.ocr.fl Fields to enable OCR highlighting for. This is required if you want to have OCR highlighting. hl.ocr.contextBlock Select which block type should be considered for determining the context. Valid values are word , line , paragraph , block or page and defaults to line . hl.ocr.contextSize Set the number of blocks above and below the matching block to be included in the passage. Defaults to 2 . hl.ocr.limitBlock Set the block type that the passage context may not exceed. Valid values are none word , line , paragraph , block or page . This value defaults to block . hl.ocr.alignSpans Ensure that the spans in the highlighted text match the text of the highlighted image parts exactly. By default ( false ), text spans will be more precise than image spans, since they can be defined at the character-level, while image spans can only be as precise as the word boundaries in the OCR. hl.ocr.pageId : Only show passages from the page with this identifier. Useful in combination with a fq for a specific document if you want to implement a \u201cSearch on this page\u201d feature (e.g. for the IIIF Content Search API ). hl.ocr.absoluteHighlights : When on , return the coordinates of highlighted regions as absolute coordinates, i.e. relative to the containing page, not the containing snippet region. Defaults to off . hl.ocr.maxPassages : Only score and rank at most this many passages to get the final list of snippets. This parameter can be used to tweak the performance. If you notice that highlighting takes a long time, you might consider lowering this value. The default is 100 . The practical result is that only the first n matches in a document will be considered for highlighting, i.e. if a more relevant passage occurs at the end of the document, it is more likely to not be considered if the total number of matches in the document exceeds this number. hl.ocr.timeAllowed : Due to the fact that generating highlighting snippets from disk can take a very long time, depending on the number of documents and snippets, you can limit the time OCR highlighting should take. The parameter takes the maximum allowed time in milliseconds . If the timeout is exceeded during highlighting, the document currently being highlighted and any other remaining documents will be skipped. The highlighting response will then only include snippets from documents that were highlighted before the timeout. The presence of partial results will be indicated by the partialOcrHighlights key in the responseHeader . hl.ocr.trackPages : When off (defaults to on ), you will not get information the containing page of a given snippet. This can improve highlighting performance, since less of the input file needs to be read and should be disabled when you index your documents at the page-level, i.e. when the identify of the page is encoded elsewhere in the document. hl.ocr.scorePassages : When off (defaults to on ), the snippets are returned in order of their occurrence in the document. Otherwise, it will follow Solr\u2019s default strategy for scoring highlighting snippets, which treats each candidate snippet as a \u2018mini-document\u2019 that is scored using TF-IDF/BM25, treating the parent document as the corpus. This results in a relevance score in relation to the parent document, i.e. the first snippet should be the most relevant snippet in the document.","title":"Available highlighting parameters"}]}